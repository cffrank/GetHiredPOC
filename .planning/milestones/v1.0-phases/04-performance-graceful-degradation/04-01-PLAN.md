---
phase: 04-performance-graceful-degradation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - packages/backend/src/utils/logger.ts
  - packages/backend/src/services/resume.service.ts
  - packages/backend/src/services/job-matching.service.ts
  - packages/backend/src/services/job-recommendations.service.ts
  - packages/backend/src/services/linkedin-parser.service.ts
  - packages/backend/package.json
autonomous: true
requirements:
  - GRACE-03
  - PERF-04

must_haves:
  truths:
    - "Uploading a multi-page PDF resume produces correctly parsed text content, not garbled binary artifacts"
    - "Backend service logs are JSON-structured with module, level, and message fields"
    - "Console calls in high-traffic services use the structured logger utility"
  artifacts:
    - path: "packages/backend/src/utils/logger.ts"
      provides: "Structured JSON logger utility for Cloudflare Workers"
      exports: ["createLogger"]
    - path: "packages/backend/src/services/resume.service.ts"
      provides: "PDF text extraction via unpdf instead of TextDecoder"
      contains: "unpdf"
  key_links:
    - from: "packages/backend/src/services/resume.service.ts"
      to: "unpdf"
      via: "import { extractText, getDocumentProxy } from 'unpdf'"
      pattern: "getDocumentProxy|extractText"
    - from: "packages/backend/src/services/job-matching.service.ts"
      to: "packages/backend/src/utils/logger.ts"
      via: "import { createLogger }"
      pattern: "createLogger"
---

<objective>
Create a structured logger utility for Cloudflare Workers and replace the broken TextDecoder PDF parsing with unpdf.

Purpose: The logger provides JSON-structured logs queryable in Cloudflare Workers Logs (replacing ad-hoc console.log strings). The unpdf switch fixes resume uploads that currently produce garbled binary output because TextDecoder cannot parse compressed PDF binary format.

Output: `packages/backend/src/utils/logger.ts` utility, unpdf installed and wired into resume.service.ts, structured logging adopted in 4 high-traffic services.
</objective>

<execution_context>
@/home/carl/.claude/get-shit-done/workflows/execute-plan.md
@/home/carl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-performance-graceful-degradation/04-RESEARCH.md

@packages/backend/src/services/resume.service.ts
@packages/backend/src/services/job-matching.service.ts
@packages/backend/src/services/job-recommendations.service.ts
@packages/backend/src/services/linkedin-parser.service.ts
@packages/backend/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create structured logger utility and install unpdf</name>
  <files>
    packages/backend/src/utils/logger.ts
    packages/backend/package.json
  </files>
  <action>
1. Create `packages/backend/src/utils/logger.ts` with a `createLogger(module: string)` factory function that returns an object with `info`, `warn`, `error`, and `debug` methods. Each method accepts `(message: string, data?: Record<string, unknown>)` and calls the corresponding `console.*` method with `JSON.stringify({ level, module, message, ...data })`. This follows the Cloudflare Workers Logs structured logging pattern — no external logging libraries.

2. Install unpdf in the backend workspace: `npm install unpdf --workspace=packages/backend`. This replaces pdf-parse for PDF text extraction. pdf-parse's browser build uses OffscreenCanvas which is NOT supported in Cloudflare Workers (confirmed: cloudflare/workerd#54).

3. Check `wrangler deploy --dry-run` output in packages/backend to verify the bundle size stays within Cloudflare Workers limits after adding unpdf. If the bundle exceeds 10MB compressed, note it — but proceed regardless (project is likely on paid tier).
  </action>
  <verify>
- `packages/backend/src/utils/logger.ts` exists and exports `createLogger`
- `npm ls unpdf --workspace=packages/backend` shows unpdf installed
- TypeScript compiles: `cd packages/backend && npx tsc --noEmit` (no new errors)
  </verify>
  <done>Logger utility exists with createLogger factory; unpdf is installed in backend workspace</done>
</task>

<task type="auto">
  <name>Task 2: Replace TextDecoder PDF parsing with unpdf and adopt structured logging</name>
  <files>
    packages/backend/src/services/resume.service.ts
    packages/backend/src/services/job-matching.service.ts
    packages/backend/src/services/job-recommendations.service.ts
    packages/backend/src/services/linkedin-parser.service.ts
  </files>
  <action>
1. In `resume.service.ts`, replace the `extractTextFromPDF` function. The current implementation uses `new TextDecoder('utf-8', { fatal: false })` followed by regex stripping — this produces garbled output for real PDFs. Replace with:
   ```typescript
   import { extractText, getDocumentProxy } from 'unpdf';
   async function extractTextFromPDF(pdfBuffer: ArrayBuffer): Promise<string> {
     const pdf = await getDocumentProxy(new Uint8Array(pdfBuffer));
     const { text } = await extractText(pdf, { mergePages: true });
     return text;
   }
   ```
   Keep the try/catch wrapper and return empty string on failure (matches existing fallback pattern from research).

2. In resume.service.ts, replace all `console.log` and `console.error` calls with the structured logger: `const logger = createLogger('resume');` then `logger.info(...)`, `logger.error(...)`.

3. In `job-matching.service.ts`, add `const logger = createLogger('job-matching');` and replace all `console.log`/`console.error`/`console.warn` calls with structured logger equivalents. Preserve the same message content but wrap in structured format.

4. In `job-recommendations.service.ts`, add `const logger = createLogger('job-recommendations');` and replace all `console.*` calls with structured logger equivalents.

5. In `linkedin-parser.service.ts`, add `const logger = createLogger('linkedin-parser');` and replace all `console.*` calls with structured logger equivalents.

Do NOT touch any other service files — scope is limited to these 4 high-traffic services that are directly involved in Phase 4 changes.
  </action>
  <verify>
- `npx tsc --noEmit` in packages/backend compiles without errors
- `grep -r "new TextDecoder" packages/backend/src/services/resume.service.ts` returns no matches (TextDecoder removed)
- `grep -r "from 'unpdf'" packages/backend/src/services/resume.service.ts` returns a match
- `grep -c "createLogger" packages/backend/src/services/resume.service.ts packages/backend/src/services/job-matching.service.ts packages/backend/src/services/job-recommendations.service.ts packages/backend/src/services/linkedin-parser.service.ts` shows 1+ match per file
  </verify>
  <done>PDF parsing uses unpdf for correct text extraction; 4 high-traffic services use structured JSON logging via createLogger</done>
</task>

</tasks>

<verification>
- TypeScript compiles cleanly in packages/backend
- No TextDecoder usage remains in resume.service.ts for PDF parsing
- unpdf is the PDF text extraction library
- Logger utility produces JSON-structured output with level, module, and message fields
- High-traffic services (resume, job-matching, job-recommendations, linkedin-parser) use structured logger
</verification>

<success_criteria>
- `extractTextFromPDF` uses unpdf's `getDocumentProxy` + `extractText` instead of TextDecoder
- `createLogger` utility exists and is imported by 4 service files
- All console.* calls in the 4 target services are replaced with logger.* calls
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-performance-graceful-degradation/04-01-SUMMARY.md`
</output>
