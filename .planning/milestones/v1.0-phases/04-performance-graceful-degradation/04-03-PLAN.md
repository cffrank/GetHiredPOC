---
phase: 04-performance-graceful-degradation
plan: 03
type: execute
wave: 2
depends_on:
  - 04-01
files_modified:
  - packages/backend/src/services/job-recommendations.service.ts
  - packages/backend/src/services/job-matching.service.ts
  - packages/backend/src/routes/jobs.ts
autonomous: true
requirements:
  - PERF-01
  - PERF-03

must_haves:
  truths:
    - "The job analysis endpoint executes 2-3 database queries instead of 7 for a single analysis request"
    - "A full recommendations call with 50 jobs executes 3-4 DB queries total, not 150+"
    - "Updating a user profile causes the next job analysis to return fresh results, not stale cached data"
  artifacts:
    - path: "packages/backend/src/services/job-recommendations.service.ts"
      provides: "Pre-loaded user context passed into analyzeJobMatch loop"
      contains: "buildUserContext"
    - path: "packages/backend/src/services/job-matching.service.ts"
      provides: "analyzeJobMatch accepts pre-loaded context; cache key includes profile version"
      contains: "updated_at"
    - path: "packages/backend/src/routes/jobs.ts"
      provides: "Single-job analysis route uses profile-versioned cache key"
      contains: "updated_at"
  key_links:
    - from: "packages/backend/src/services/job-recommendations.service.ts"
      to: "packages/backend/src/services/job-matching.service.ts"
      via: "buildUserContext called once, passed to analyzeJobMatch in loop"
      pattern: "buildUserContext|userContext"
    - from: "packages/backend/src/services/job-matching.service.ts"
      to: "KV_CACHE"
      via: "Cache key includes updated_at for profile version"
      pattern: "v\\$\\{.*updated_at"
---

<objective>
Eliminate N+1 database queries in job recommendations and fix stale cache after profile updates.

Purpose: `analyzeJobMatch` fires 3+ DB queries per job in the recommendation loop — with 50 jobs that's 150+ queries. Pre-loading user context once before the loop reduces this to 3-4 total. The cache key `match:{userId}:{jobId}` never invalidates when the profile changes — embedding `updated_at` in the key makes old entries unreachable (they expire via 7-day TTL).

Output: `buildUserContext` function pre-loads profile data once; `analyzeJobMatch` accepts pre-loaded context; cache keys include profile version; stale results are never served after profile updates.
</objective>

<execution_context>
@/home/carl/.claude/get-shit-done/workflows/execute-plan.md
@/home/carl/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-performance-graceful-degradation/04-RESEARCH.md
@.planning/phases/04-performance-graceful-degradation/04-01-SUMMARY.md

@packages/backend/src/services/job-recommendations.service.ts
@packages/backend/src/services/job-matching.service.ts
@packages/backend/src/routes/jobs.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pre-load user context and eliminate N+1 queries</name>
  <files>
    packages/backend/src/services/job-recommendations.service.ts
    packages/backend/src/services/job-matching.service.ts
  </files>
  <action>
1. In `job-recommendations.service.ts`, create a `buildUserContext` function that loads user profile data in parallel using `Promise.all`:
   ```typescript
   async function buildUserContext(env: Env, userId: string) {
     const [user, workHistory, education] = await Promise.all([
       env.DB.prepare('SELECT * FROM users WHERE id = ?').bind(userId).first(),
       env.DB.prepare('SELECT company, title, description FROM work_experience WHERE user_id = ? ORDER BY start_date DESC LIMIT 3').bind(userId).all(),
       env.DB.prepare('SELECT school, degree, field_of_study FROM education WHERE user_id = ? ORDER BY start_date DESC LIMIT 2').bind(userId).all(),
     ]);
     return { user, workHistory: workHistory.results, education: education.results };
   }
   ```
   Adjust the column names and query shapes to match what the existing `analyzeJobMatch` function currently queries. Read the current code carefully to match exact column lists and table structure.

2. In the recommendations loop (likely `getTopJobRecommendations` or similar), call `buildUserContext` ONCE before the loop, then pass the result into each `analyzeJobMatch` call.

3. In `job-matching.service.ts`, modify `analyzeJobMatch` to accept an optional `userContext` parameter. When provided, skip the individual DB queries for user/work_experience/education and use the pre-loaded data. When NOT provided (for single-job analysis from the `/api/jobs/:id/analyze` route), load the data inline as before — this preserves backward compatibility.

4. In `getRecommendationsWithJobDetails` (or equivalent), check if the function re-fetches jobs by ID after recommendations are computed. If so, reuse the original jobs list instead of querying again. The research confirms this is an unnecessary additional query.

5. Use the structured logger from 04-01 (`createLogger('job-recommendations')` / `createLogger('job-matching')`) if the structured logging is already in place from plan 01. If the logger calls are already there, keep them. If plan 01 hasn't run yet, use console.log — plan 01 will convert them.

**PITFALL WARNING (from research):** Keep `analyzeJobMatch` backward-compatible. The `/api/jobs/:id/analyze` route calls it for single-job analysis and should continue to work without pre-loaded context. Use optional parameter with fallback to inline loading.
  </action>
  <verify>
- `grep "buildUserContext" packages/backend/src/services/job-recommendations.service.ts` shows the function exists
- `grep "userContext" packages/backend/src/services/job-matching.service.ts` shows the parameter is accepted
- Count DB queries in the recommendation flow: trace through the code and confirm that the loop no longer contains per-job DB queries for user profile data
- `cd packages/backend && npx tsc --noEmit` compiles without errors
  </verify>
  <done>Recommendation loop executes 3-4 DB queries total (user+workHistory+education in parallel, jobs list) instead of 150+ for 50 jobs</done>
</task>

<task type="auto">
  <name>Task 2: Fix cache invalidation with profile-versioned keys</name>
  <files>
    packages/backend/src/services/job-matching.service.ts
    packages/backend/src/routes/jobs.ts
  </files>
  <action>
1. In `job-matching.service.ts`, find the cache key construction (currently `match:{userId}:{jobId}` or similar). Modify to include the user's `updated_at` timestamp:
   ```typescript
   const profileVersion = userProfile.updated_at || 0;
   const cacheKey = `match:${userId}:${job.id}:v${profileVersion}`;
   ```
   When `userContext` is provided (from recommendation loop), use `userContext.user.updated_at`. When loading inline (single-job analysis), use the freshly-queried user's `updated_at`.

2. In `jobs.ts`, find the `/api/jobs/:id/analyze` route handler. It likely uses a cache key like `job-analysis:{userId}:{jobId}`. Apply the same fix — include `updated_at` in the cache key. The user object should already be available from the auth middleware (`c.get('user')`) — check if it includes `updated_at`. If not, query for it.

3. Do NOT attempt to delete old cache entries. Old keys with the previous `updated_at` value become orphaned and expire via the existing 7-day KV TTL. This is the correct pattern per Cloudflare docs — KV does not support efficient prefix-based key deletion.

4. Verify that the KV_CACHE.put calls also use the new versioned key (both the read and write sides must use the same key format).
  </action>
  <verify>
- `grep "updated_at" packages/backend/src/services/job-matching.service.ts` shows updated_at in cache key construction
- `grep "updated_at" packages/backend/src/routes/jobs.ts` shows updated_at in the analyze route's cache key
- `cd packages/backend && npx tsc --noEmit` compiles without errors
- Cache key format includes version suffix (pattern: `:v${...updated_at}`)
  </verify>
  <done>Cache keys include profile version (updated_at); profile updates cause cache misses forcing fresh analysis; old entries expire naturally via TTL</done>
</task>

</tasks>

<verification>
- TypeScript compiles cleanly in packages/backend
- Recommendation loop pre-loads user context once, not per-job
- Single-job analysis route still works (backward-compatible analyzeJobMatch)
- Cache keys include profile updated_at timestamp
- No manual cache deletion — relies on TTL expiry for old keys
</verification>

<success_criteria>
- A recommendations call with 50 jobs executes ~3-4 DB queries, not 150+
- Single-job analysis via `/api/jobs/:id/analyze` still works correctly
- Updating profile and re-running analysis returns fresh (non-cached) results
- No TypeScript compilation errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-performance-graceful-degradation/04-03-SUMMARY.md`
</output>
